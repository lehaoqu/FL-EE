alg: inclusivefl
policy: boosted
dataset: qnli
model: bert
config_path: models/google-bert/bert-12-128-uncased
ft: lora
load_path: 
seed: 1117
total_num: 100
sr: 0.1
suffix: exps/BASE_GLUE/lora_boosted/qnli
device: 3
rnd: 500
bs: 32
epoch: 1
lr: 0.05
gamma: 0.99
optim: sgd
valid_gap: 10
eq_ratios: (0.25, 0.25, 0.25, 0.25)
eq_depths: (3, 6, 9, 12)
multi_exit: False
if_mode: all
cosine: False
valid_ratio: 0.2
eval_models_dir: script/0818-1e-1
ensemble_weight: 0.2
output: <_io.TextIOWrapper name='./exps/BASE_GLUE/lora_boosted/qnli/inclusivefl_qnli_bert_100c_1E_lrsgd0.05_boosted.txt' mode='a' encoding='UTF-8'>
========== Round 0 ==========
server, accuracy: 49.79, exits:['49.78', '49.78', '49.78', '49.82'] loss: 1.91
========== Round 10 ==========
server, accuracy: 62.09, exits:['49.78', '61.01', '65.78', '71.80'] loss: 1.82
========== Round 20 ==========
server, accuracy: 65.16, exits:['49.78', '73.95', '65.74', '71.19'] loss: 1.64
