alg: darkflpg
policy: boosted
dataset: speechcmds
model: vit
config_path: models/facebook/deit-tiny-patch16-224
ft: lora
load_path: 
seed: 1117
total_num: 100
sr: 0.1
suffix: exps/test/lora/lora_boosted
device: 3
rnd: 500
bs: 32
epoch: 1
lr: 0.05
gamma: 0.99
optim: sgd
valid_gap: 10
eq_ratios: (0.25, 0.25, 0.25, 0.25)
eq_depths: (3, 6, 9, 12)
multi_exit: False
if_mode: all
cosine: False
valid_ratio: 0.2
eval_models_dir: script/0818-1e-1
is_latent: True
s_epoches: 2
s_bs: 32
adaptive_epoches: False
kd_skip: 1
kd_begin: 0
kd_lr: 0.05
kd_response_ratio: 3
kd_dist_ratio: 5
kd_angle_ratio: 10
kd_dark_ratio: 0
kd_n_iters: 5
kd_gap: 1
g_skip: 1
g_begin: 0
g_lr: 0.01
g_y: 1
g_div: 1
g_gap: 0
g_diff: 1
g_n_iters: 1
kd_direction: sl
kd_join: last
agg: after
loss_type: kd
dm: loss
diff_client_gap: 1
diff_generator: True
sw: learn
sw_type: soft
exit_p: 30
s_gamma: 1
ensemble_weight: 0.2
output: <_io.TextIOWrapper name='./exps/test/lora/lora_boosted/darkflpg_speechcmds_vit_100c_1E_lrsgd0.05_boosted.txt' mode='a' encoding='UTF-8'>
========== Round 0 ==========
server, accuracy: 4.25, exits:['3.53', '3.53', '3.45', '6.49'] loss: 11.39
alg: darkflpg
policy: boosted
dataset: speechcmds
model: vit
config_path: models/facebook/deit-tiny-patch16-224
ft: lora
load_path: 
seed: 1117
total_num: 100
sr: 0.1
suffix: exps/test/lora/lora_boosted
device: 3
rnd: 500
bs: 32
epoch: 1
lr: 0.05
gamma: 0.99
optim: sgd
valid_gap: 10
eq_ratios: (0.25, 0.25, 0.25, 0.25)
eq_depths: (3, 6, 9, 12)
multi_exit: False
if_mode: all
cosine: False
valid_ratio: 0.2
eval_models_dir: script/0818-1e-1
is_latent: True
s_epoches: 2
s_bs: 32
adaptive_epoches: False
kd_skip: 1
kd_begin: 0
kd_lr: 0.05
kd_response_ratio: 3
kd_dist_ratio: 5
kd_angle_ratio: 10
kd_dark_ratio: 0
kd_n_iters: 5
kd_gap: 1
g_skip: 1
g_begin: 0
g_lr: 0.01
g_y: 1
g_div: 1
g_gap: 0
g_diff: 1
g_n_iters: 1
kd_direction: sl
kd_join: last
agg: after
loss_type: kd
dm: loss
diff_client_gap: 1
diff_generator: True
sw: learn
sw_type: soft
exit_p: 30
s_gamma: 1
ensemble_weight: 0.2
output: <_io.TextIOWrapper name='./exps/test/lora/lora_boosted/darkflpg_speechcmds_vit_100c_1E_lrsgd0.05_boosted.txt' mode='a' encoding='UTF-8'>
========== Round 0 ==========
server, accuracy: 5.21, exits:['3.69', '3.69', '6.43', '7.01'] loss: 9.95
alg: darkflpg
policy: boosted
dataset: speechcmds
model: vit
config_path: models/facebook/deit-tiny-patch16-224
ft: lora
load_path: 
seed: 1117
total_num: 100
sr: 0.1
suffix: exps/test/lora/lora_boosted
device: 3
rnd: 500
bs: 32
epoch: 1
lr: 0.05
gamma: 0.99
optim: sgd
valid_gap: 10
eq_ratios: (0.25, 0.25, 0.25, 0.25)
eq_depths: (3, 6, 9, 12)
multi_exit: False
if_mode: all
cosine: False
valid_ratio: 0.2
eval_models_dir: script/0818-1e-1
is_latent: True
s_epoches: 2
s_bs: 32
adaptive_epoches: False
kd_skip: 1
kd_begin: 0
kd_lr: 0.05
kd_response_ratio: 3
kd_dist_ratio: 5
kd_angle_ratio: 10
kd_dark_ratio: 0
kd_n_iters: 5
kd_gap: 1
g_skip: 1
g_begin: 0
g_lr: 0.01
g_y: 1
g_div: 1
g_gap: 0
g_diff: 1
g_n_iters: 1
kd_direction: sl
kd_join: last
agg: after
loss_type: kd
dm: loss
diff_client_gap: 1
diff_generator: True
sw: learn
sw_type: soft
exit_p: 30
s_gamma: 1
ensemble_weight: 0.2
output: <_io.TextIOWrapper name='./exps/test/lora/lora_boosted/darkflpg_speechcmds_vit_100c_1E_lrsgd0.05_boosted.txt' mode='a' encoding='UTF-8'>
========== Round 0 ==========
server, accuracy: 4.25, exits:['3.53', '3.53', '3.45', '6.49'] loss: 11.39
========== Round 10 ==========
server, accuracy: 45.78, exits:['3.67', '48.24', '66.21', '65.00'] loss: 7.33
========== Round 20 ==========
server, accuracy: 72.67, exits:['52.10', '78.16', '80.89', '79.51'] loss: 3.39
========== Round 30 ==========
server, accuracy: 80.55, exits:['69.96', '84.33', '84.46', '83.44'] loss: 2.30
